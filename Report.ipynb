{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Agent\n",
    "\n",
    "The learning agent is double, deep q-network agent, that uses prioritised replay. An outline of this agent is the following:\n",
    "\n",
    " * There are two networks: one used for control, and one used for policy improvement (target).\n",
    " * Each step of an episode, an epsilon-greedy policy is use for control of the agent. This epsilon-greedy policy is based on a deep neural network that approximates the action value for each state action pair.\n",
    " * At the end of each step, the s, a, s, r tuple is added to the replay buffer along with a priority, which is calculated as the magnitude of the difference between the control and the target network, i.e., the error in boostrapping Bellman eqn for the current state-action pair. Only a fixed number of experiences are retained in the buffer and the ones with the smallest priority are dropped first.\n",
    " * Periodically, the control network is update with by trying to minimise the loss in the Bellman equations for a sample of experiences from the replay buffer. The probabilities of these samples are proportional to their priorities. At this time the target network is moved toward the control network with an exponential filter, controlled by the rate tau. The contribution of each experience to the loss must be adjusted however to compensate for the non-uniform probability of selection. See Agent.learn for the details.\n",
    " * At the end of the episode the value of epsilon is decreased by a constant factor.\n",
    " \n",
    "All of the hyper-parameters were chosen based on what worked well in the simpler taxi environment.\n",
    " \n",
    "### Q Network\n",
    "\n",
    "The Q network has an input layer that consists of the continuous state variables. The output layer consists of the state-action values for each of the discrete choices of action. The greedy policy uses the action corresponding to the max of the output layer as its action. There are two hidden layers consisting of 64-unit linear function and a ELU activation.\n",
    "\n",
    "\n",
    "## Further work\n",
    "\n",
    " * Little effort was to innovate on the q-network. More sophisticated techniques could be used there, e.g., dropout, batch normalisation, adding a regularisation loss.\n",
    " * The original prioritised replay buffer (https://arxiv.org/pdf/1511.05952.pdf) introduces two more hyper-parameters to control the weighting by priority. These were not used and could be introduced. However adding more hyper-parameters means more tuning is required and there is greater possibility of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from reacher_env import make_reacher_env, reacher_episode\n",
    "from run import train\n",
    "from agent import Agent\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = make_reacher_env()\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset()[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "\n",
    "agent = Agent(\n",
    "    device,\n",
    "    state_size,\n",
    "    action_size,\n",
    "    buffer_size=int(1e6),\n",
    "    batch_size=64,\n",
    "    actor_learning_rate=1e-4,\n",
    "    critic_learning_rate=1e-3,\n",
    "    discount_rate=0.99,\n",
    "    tau=1e-3,\n",
    "    steps_per_update=5,\n",
    "    weight_decay=0.00,\n",
    "    noise_decay=1.0,\n",
    "    noise_max=0.2,\n",
    "    dropout_p=0.2,\n",
    "    n_agents=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = train(env, agent, brain_name, max_eps=int(2e5), min_score=30.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of scores, and mean scores over 100 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_scores = np.array(scores).reshape(-1, 100).mean(axis=1)\n",
    "# plt.plot(scores)\n",
    "# plt.plot(np.arange(avg_scores.shape[0]) * 100 + 50, avg_scores, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.970999464206404"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.restore('best')\n",
    "\n",
    "reacher_episode(env, agent, env.brain_names[0], train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
